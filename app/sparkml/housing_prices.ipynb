{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6226fff0",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ad772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(os.listdir('work'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0cb49",
   "metadata": {},
   "source": [
    "### Step 1: Create Spark object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb99e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No environment path hacking required!\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HousingModelDocker\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a914f31",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "display(os.listdir('work'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad51683",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('work/housing_prices.csv',header=True, inferSchema=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37191b92",
   "metadata": {},
   "source": [
    "### Step 3: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9c5a8",
   "metadata": {},
   "source": [
    "#### Counting NULL values in SINGLE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b641d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data.select\n",
    "    (\n",
    "        F.count(\n",
    "                F.when(\n",
    "                        F.col('ocean_proximity').isNull(),1\n",
    "                    )\n",
    "                ).alias('ocean_proximity')\n",
    "    ).show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc58cd",
   "metadata": {},
   "source": [
    "#### Counting NULL values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data.select\n",
    "    (\n",
    "        [\n",
    "            F.count(\n",
    "                F.when(\n",
    "                        F.col(c).isNull(),1\n",
    "                    )\n",
    "                ).alias(c) for c in data.columns\n",
    "        ]\n",
    "    ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb2138",
   "metadata": {},
   "source": [
    "#### Handling NULL value in ``total_bedrooms`` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb98c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.dropna(subset=['total_bedrooms'])\n",
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826af177",
   "metadata": {},
   "source": [
    "### Step 4: Split dataframe using ``randomSplit([0.8,0.2], seed=42)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = filtered_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print('Train size ', train_data.count())\n",
    "print('Test size ', test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efee2b4",
   "metadata": {},
   "source": [
    "### Step 5: Create Features using VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = ['housing_median_age',\n",
    "                   'total_rooms',\n",
    "                   'total_bedrooms',\n",
    "                   'population',\n",
    "                   'households',\n",
    "                   'median_income']\n",
    "assemblers = VectorAssembler(inputCols=feature_columns,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbfd6e9",
   "metadata": {},
   "source": [
    "### Create training data\n",
    "\n",
    "- use train_data from the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_trained_data = assemblers.transform(train_data)\n",
    "transformed_trained_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a62dd",
   "metadata": {},
   "source": [
    "### Create testing data\n",
    "\n",
    "- use test_data from the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114506d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_data = assemblers.transform(test_data)\n",
    "transformed_test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cadb4",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='median_house_value', regParam=0.001)\n",
    "model = lr.fit(transformed_trained_data)\n",
    "\n",
    "predictions = model.transform(transformed_test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e701fb",
   "metadata": {},
   "source": [
    "### Evaluate using \n",
    "\n",
    "- Mean Absolute Error\n",
    "\n",
    "Imagine you're guessing how many candies are in a big jar.\n",
    "\n",
    "You guess 10 candies. There are actually 12 candies.\n",
    "\n",
    "Your mistake is 2 candies too few.\n",
    "\n",
    "Your friend guesses 15 candies. They are 5 candies too many.\n",
    "\n",
    "Now, Mean Absolute Error (MAE) is like asking: \"On average, how far off were our guesses?\"\n",
    "\n",
    "Step 1: Take the size of each mistake (ignore if too high or too low):\n",
    "\n",
    "Your mistake: 2 candies\n",
    "\n",
    "Friend's mistake: 5 candies\n",
    "\n",
    "Step 2: Average them: (2 + 5) √∑ 2 = 3.5 candies\n",
    "\n",
    "MAE = 3.5\n",
    "\n",
    "That means: \"Our guesses were wrong by 3.5 candies on average.\"\n",
    "\n",
    "Lower MAE = Better guessing! üéØ\n",
    "\n",
    "Real example: If your model predicts house prices, MAE = $20,000 means \"On average, my predictions are off by $20,000.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9035d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 56292.33482622761\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol='median_house_value', predictionCol='prediction', metricName='mae')\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c3c7b",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- Mean Absolute Error\n",
    "\n",
    "Imagine you're guessing how many candies are in a big jar.\n",
    "\n",
    "You guess 10 candies. There are actually 12 candies.\n",
    "\n",
    "Your mistake is 2 candies too few.\n",
    "\n",
    "Your friend guesses 15 candies. They are 5 candies too many.\n",
    "\n",
    "Now, Mean Absolute Error (MAE) is like asking: \"On average, how far off were our guesses?\"\n",
    "\n",
    "Step 1: Take the size of each mistake (ignore if too high or too low):\n",
    "\n",
    "Your mistake: 2 candies\n",
    "\n",
    "Friend's mistake: 5 candies\n",
    "\n",
    "Step 2: Average them: (2 + 5) √∑ 2 = 3.5 candies\n",
    "\n",
    "MAE = 3.5\n",
    "\n",
    "That means: \"Our guesses were wrong by 3.5 candies on average.\"\n",
    "\n",
    "Lower MAE = Better guessing! üéØ\n",
    "\n",
    "Real example: If your model predicts house prices, MAE = $20,000 means \"On average, my predictions are off by $20,000.\"\n",
    "\n",
    "- Root Mean Squared Error\n",
    "\n",
    "You guess: 10 candies (actual: 12) ‚Üí Mistake = 2\n",
    "Friend guesses: 15 candies (actual: 12) ‚Üí Mistake = 3\n",
    "\n",
    "MAE (like before):\n",
    "Just average: (2 + 3) √∑ 2 = 2.5 candies ‚ùå\n",
    "\n",
    "RMSE (punishes big mistakes):\n",
    "Step 1: Square the mistakes (big ones hurt more!)\n",
    "\n",
    "Your mistake: 2¬≤ = 4\n",
    "\n",
    "Friend's mistake: 3¬≤ = 9\n",
    "Step 2: Average: (4 + 9) √∑ 2 = 6.5\n",
    "Step 3: Square root: ‚àö6.5 ‚âà 2.55 candies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10315cf3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
